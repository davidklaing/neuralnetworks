{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Using neural nets to recognize handwritten digits\n",
    "\n",
    "Starting with definition of artificial neur*ons*, i.e. perceptrons and sigmoid neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "A perceptron is a simple neuron (a node in the neural network) that takes $n$ binary inputs and returns a binary output. It usually attaches weights to each input, and if the incoming signals, multiplied by their respective weights, are higher than a given threshold, the output signal is fired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class perceptron:\n",
    "    def fire(self, inputs, weights, threshold):\n",
    "        return 1 if np.sum(inputs*weights) > threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_neuron = perceptron()\n",
    "my_neuron.fire(\n",
    "    inputs = np.array([1, 1]), \n",
    "    weights = np.array([0.6, 0.5]), \n",
    "    threshold = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By stacking such neurons in multiple layers, the neural network can make increasingly subtle decisions.\n",
    "\n",
    "For notational simplicity that will become clear later on, we use instead of a *threshold* a *bias* term, which describes something like \"the difficulty of getting this neuron to fire.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def fire(self, inputs, weights, bias):\n",
    "        return 1 if np.dot(inputs,weights) + bias > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_neuron = perceptron()\n",
    "my_new_neuron.fire(\n",
    "    inputs = np.array([0, 0, 1]), \n",
    "    weights = np.array([2, 2, 6]), \n",
    "    bias = -5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons can be configured to compute logical functions, such as \"and\", \"or\", or \"nand\". For example, here is a \"nand\" perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class nand_perceptron:\n",
    "    weights = np.array([-2, -2])\n",
    "    bias = 3\n",
    "    def fire(self, inputs, weights = weights, bias = bias):\n",
    "        return 1 if np.dot(inputs,weights) + bias > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nand_neuron = nand_perceptron()\n",
    "nand_neuron.fire(inputs = np.array([1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty cool, because NAND is universal for computation. That is, we can build up any computation out of NAND gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classic_nand_gate:\n",
    "    def fire(self, inputs):\n",
    "        return False if inputs[0] and inputs[1] else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_nand_gate = classic_nand_gate()\n",
    "my_nand_gate.fire(inputs = [True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can build up bitwise addition out of NAND gates. And since we can build up NAND out of a perceptron, perceptrons are universal for computation too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_bits(bit1, bit2, type_of_gate):\n",
    "    \n",
    "    if type_of_gate == \"classic\":\n",
    "        gate = classic_nand_gate()\n",
    "    elif type_of_gate == \"perceptron\":\n",
    "        gate = nand_perceptron()\n",
    "    \n",
    "    gate0_in = [bit1, bit2]\n",
    "    gate0_out = gate.fire(inputs = gate0_in)\n",
    "    \n",
    "    gate1_in = [bit1, gate0_out]\n",
    "    gate1_out = gate.fire(inputs = gate1_in)\n",
    "    \n",
    "    gate2_in = [gate0_out, bit2]\n",
    "    gate2_out = gate.fire(inputs = gate2_in)\n",
    "    \n",
    "    gate3_in = [gate1_out, gate2_out]\n",
    "    gate3_out = gate.fire(inputs = gate3_in)\n",
    "    \n",
    "    gate4_in = [gate0_out, gate0_out]\n",
    "    gate4_out = gate.fire(inputs = gate4_in)\n",
    "    \n",
    "    bit_sum = gate3_out\n",
    "    carry_bit = gate4_out\n",
    "    \n",
    "    return str(int(carry_bit)) + str(int(bit_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert add_bits(1, 1, type_of_gate=\"classic\") == '10'\n",
    "assert add_bits(0, 1, type_of_gate=\"classic\") == '01'\n",
    "assert add_bits(1, 0, type_of_gate=\"classic\") == '01'\n",
    "assert add_bits(0, 0, type_of_gate=\"classic\") == '00'\n",
    "\n",
    "assert add_bits(1, 1, type_of_gate=\"perceptron\") == '10'\n",
    "assert add_bits(0, 1, type_of_gate=\"perceptron\") == '01'\n",
    "assert add_bits(1, 0, type_of_gate=\"perceptron\") == '01'\n",
    "assert add_bits(0, 0, type_of_gate=\"perceptron\") == '00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't so exciting on its own. BUT, because perceptrons have weights and bias values that can be tuned to *configure* lower-level logical functions that can in turn be combined to configure higher-level computations, we should be able to expose an artificial neural network to an external source of data that will tune those weights and biases for us. We can then end up with a neural network that is trained to represent any arbitrary computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
