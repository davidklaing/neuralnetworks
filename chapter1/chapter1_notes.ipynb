{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Using neural nets to recognize handwritten digits\n",
    "\n",
    "Starting with definition of artificial neur*ons*, i.e. perceptrons and sigmoid neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "A perceptron is a simple neuron (a node in the neural network) that takes $n$ binary inputs and returns a binary output. It usually attaches weights to each input, and if the incoming signals, multiplied by their respective weights, are higher than a given threshold, the output signal is fired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class perceptron:\n",
    "    def fire(self, inputs, weights, threshold):\n",
    "        return 1 if np.sum(inputs*weights) > threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_neuron = perceptron()\n",
    "my_neuron.fire(\n",
    "    inputs = np.array([1, 1]), \n",
    "    weights = np.array([0.6, 0.5]), \n",
    "    threshold = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By stacking such neurons in multiple layers, the neural network can make increasingly subtle decisions.\n",
    "\n",
    "For notational simplicity that will become clear later on, we use instead of a *threshold* a *bias* term, which describes something like \"the difficulty of getting this neuron to fire.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def fire(self, inputs, weights, bias):\n",
    "        return 1 if np.dot(inputs,weights) + bias > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_neuron = perceptron()\n",
    "my_new_neuron.fire(\n",
    "    inputs = np.array([0, 0, 1]), \n",
    "    weights = np.array([2, 2, 6]), \n",
    "    bias = -5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons can be configured to compute logical functions, such as \"and\", \"or\", or \"nand\". For example, here is a \"nand\" perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class nand_perceptron:\n",
    "    weights = np.array([-2, -2])\n",
    "    bias = 3\n",
    "    def fire(self, inputs, weights = weights, bias = bias):\n",
    "        return 1 if np.dot(inputs,weights) + bias > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nand_neuron = nand_perceptron()\n",
    "nand_neuron.fire(inputs = np.array([1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty cool, because NAND is universal for computation. That is, we can build up any computation out of NAND gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classic_nand_gate:\n",
    "    def fire(self, inputs):\n",
    "        return False if inputs[0] and inputs[1] else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_nand_gate = classic_nand_gate()\n",
    "my_nand_gate.fire(inputs = [True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can build up bitwise addition out of NAND gates. And since we can build up NAND out of a perceptron, perceptrons are universal for computation too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_bits(bit1, bit2, type_of_gate):\n",
    "    \n",
    "    if type_of_gate == \"classic\":\n",
    "        gate = classic_nand_gate()\n",
    "    elif type_of_gate == \"perceptron\":\n",
    "        gate = nand_perceptron()\n",
    "    \n",
    "    gate0_in = [bit1, bit2]\n",
    "    gate0_out = gate.fire(inputs = gate0_in)\n",
    "    \n",
    "    gate1_in = [bit1, gate0_out]\n",
    "    gate1_out = gate.fire(inputs = gate1_in)\n",
    "    \n",
    "    gate2_in = [gate0_out, bit2]\n",
    "    gate2_out = gate.fire(inputs = gate2_in)\n",
    "    \n",
    "    gate3_in = [gate1_out, gate2_out]\n",
    "    gate3_out = gate.fire(inputs = gate3_in)\n",
    "    \n",
    "    gate4_in = [gate0_out, gate0_out]\n",
    "    gate4_out = gate.fire(inputs = gate4_in)\n",
    "    \n",
    "    bit_sum = gate3_out\n",
    "    carry_bit = gate4_out\n",
    "    \n",
    "    return str(int(carry_bit)) + str(int(bit_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert add_bits(1, 1, type_of_gate=\"classic\") == '10'\n",
    "assert add_bits(0, 1, type_of_gate=\"classic\") == '01'\n",
    "assert add_bits(1, 0, type_of_gate=\"classic\") == '01'\n",
    "assert add_bits(0, 0, type_of_gate=\"classic\") == '00'\n",
    "\n",
    "assert add_bits(1, 1, type_of_gate=\"perceptron\") == '10'\n",
    "assert add_bits(0, 1, type_of_gate=\"perceptron\") == '01'\n",
    "assert add_bits(1, 0, type_of_gate=\"perceptron\") == '01'\n",
    "assert add_bits(0, 0, type_of_gate=\"perceptron\") == '00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't so exciting on its own. BUT, because perceptrons have weights and bias values that can be tuned to *configure* lower-level logical functions that can in turn be combined to configure higher-level computations, we should be able to expose an artificial neural network to an external source of data that will tune those weights and biases for us. We can then end up with a neural network that is trained to represent any arbitrary computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be able to train our neural network by slightly tuning the weights and biases of its constituent neurons, we need small changes in tunings to cause small changes in outputs. Perceptrons make this hard, because they can only output 0 or 1. There's no middle ground. That's why we'll benefit from using *sigmoid neurons* instead.\n",
    "\n",
    "The output of a sigmoid neuron is not 0 or 1, but $\\sigma(w\\cdot x + b)$, where $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "(In this case $\\sigma()$ is an example of an *activation function* $f()$. We'll see other examples later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class sigmoid_neuron:\n",
    "    def fire(self, inputs, weights, bias):\n",
    "        return 1 / (1 + math.exp(-np.dot(inputs, weights) - bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01798620996209156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sig_neuron = sigmoid_neuron()\n",
    "my_sig_neuron.fire(inputs=np.array([1, 2, 3]), weights=np.array([1, 1, 1]), bias=-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### Sigmoid neurons simulating perceptrons, part I\n",
    "\n",
    "Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, $c>0$. Show that the behaviour of the network doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_perceptron = perceptron()\n",
    "inputs = np.array([1, 2, 3])\n",
    "weights = np.array([3, 3, 4])\n",
    "bias = -3\n",
    "\n",
    "for positive_constant in range(1, 101):\n",
    "    original = my_perceptron.fire(inputs = inputs, weights = weights, bias = bias)\n",
    "    modified = my_perceptron.fire(inputs = inputs, weights = positive_constant*weights, bias = positive_constant*bias)\n",
    "    assert original == modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a perceptron network is made up of only perceptrons, we can show that the network's overall behaviour will never change so long as any individual perceptron's behaviour will never change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for a perceptron:\n",
    "\n",
    "$1$ if $w\\cdot x + b > 0$\n",
    "\n",
    "$0$ if $w\\cdot x + b <= 0$\n",
    "\n",
    "Let's expand out the dot product and see whether it's possible to change the sign of $cw\\cdot x + cb$ just by varying $c$.\n",
    "\n",
    "$cw\\cdot x + cb = c\\sum_{j}w_{j}x_{j} + cb$\n",
    "\n",
    "$cw\\cdot x + cb = cw_{1}x_{1} + cw_{2}x_{2} + ... + cw_{j}x_{j} + cb$\n",
    "\n",
    "$cw\\cdot x + cb = c(w_{1}x_{1} + w_{2}x_{2} + ... + w_{j}x_{j} + b)$\n",
    "\n",
    "Since $c$ is always a positive constant, the sign of the right-hand side will never change. Therefore, the behaviour of a neural network made up of perceptrons will never change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sigmoid neurons simulating perceptrons, part II\n",
    "\n",
    "Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w\\cdot x+b \\neq 0$ for the input $x$ to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant $c>0$. Show that in the limit as $c\\to\\infty$ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w⋅x+b=0$ for one of the perceptrons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So we're trying to show that this expression behaves exactly like a percetron as $c\\to\\infty$:\n",
    "\n",
    "$\\lim_{c\\to\\infty} \\sigma(cw\\cdot x + cb)$\n",
    "\n",
    "$\\lim_{c\\to\\infty} \\sigma(c(w\\cdot x + b))$\n",
    "\n",
    "$\\lim_{c\\to\\infty} \\frac{1}{1 + e^{c(-w\\cdot x - b))}}$\n",
    "\n",
    "As $c\\to\\infty$, this goes to 0 if $-w\\cdot x - b > 0$, and to 1 if $-w\\cdot x - b < 0$, which is exactly the behaviour of a perceptron. (Technically, the perceptron outputs 0 if $w\\cdot x + b < 0$ and 1 if $w\\cdot x + b > 0$, but these statements are equivalent to the previous ones because $-1*1=-1$ and $-1*-1=1$.)\n",
    "\n",
    "This fails when $w\\cdot x+b=0$ because $\\lim_{c\\to\\infty} \\frac{1}{1 + e^{c*0}} = 0.5$, which is an impossible output for a perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The architecture of neural networks\n",
    "\n",
    "Neural networks which use the output from one layer as the input to the next are called *feedforward* neural networks, and they will be the focus of the book. (See also *recurrent* neural networks, which allow for feedback loops within the network.)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.\n",
    "\n",
    "![](architecture_exercise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bitwise representations of the numbers from 0 to 9:\n",
    "\n",
    "- 0: 0000\n",
    "- 1: 0001\n",
    "- 2: 0010\n",
    "- 3: 0011\n",
    "- 4: 0100\n",
    "- 5: 0101\n",
    "- 6: 0110\n",
    "- 7: 0111\n",
    "- 8: 1000\n",
    "- 9: 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_output_layer(input_vector, weights_matrix, bias_vector):\n",
    "    return(np.sum(input_vector*weights_matrix, axis=1) + bias_vector > 0)\n",
    "\n",
    "weights_matrix = np.array([[0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "                           [0, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "                           [0, 0, 0, 0, 1, 1, 1, 1, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0, 1, 1]])\n",
    "bias_vector = np.array([0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    0: {\n",
    "        \"input_vector\": np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 0, 0, 0])\n",
    "    },\n",
    "    1: {\n",
    "        \"input_vector\": np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 0, 0, 0])\n",
    "    },\n",
    "    2: {\n",
    "        \"input_vector\": np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 1, 0, 0])\n",
    "    },\n",
    "    3: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 1, 0, 0])\n",
    "    },\n",
    "    4: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 0, 1, 0])\n",
    "    },\n",
    "    5: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 0, 1, 0])\n",
    "    },\n",
    "    6: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 1, 1, 0])\n",
    "    },\n",
    "    7: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 1, 1, 0])\n",
    "    },\n",
    "    8: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
    "        \"output_vector\": np.array([0, 0, 0, 1])\n",
    "    },\n",
    "    9: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
    "        \"output_vector\": np.array([1, 0, 0, 1])\n",
    "    }\n",
    "}\n",
    "\n",
    "for digit in range(10):\n",
    "    output = new_output_layer(\n",
    "        input_vector=mappings[digit][\"input_vector\"], \n",
    "        weights_matrix=weights_matrix, \n",
    "        bias_vector=bias_vector\n",
    "    )\n",
    "    expected_output = mappings[digit][\"output_vector\"]\n",
    "    assert (output == expected_output).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Learning with gradient descent\n",
    "\n",
    "To quantify how well a given set of weights and biases in a neural network gives us the output we want, we define a cost function:\n",
    "\n",
    "$$C(w, b) = \\frac{1}{2n}\\sum_x||y(x) - a||^2$$\n",
    "\n",
    "$w$ is the collection of all the weights in the network, $b$ is all the biases, $n$ is the number of training inputs, $a$ is the vector of outputs from the network when $x$ is the input, and the sum is over all training inputs, $x$.\n",
    "\n",
    "We want to minimize the value of this function (because the only way to make $C$ low is to make $a$ as close as possible to $y(x)$). We do this using gradient descent.\n",
    "\n",
    "The *gradient* of C is the vector partial derivatives of C with respect to each of the variables it depends on. In the case of just two variables, $v_1$ and $v_2$, it looks like this:\n",
    "\n",
    "$$\\nabla C = \\left(\\frac{\\partial C}{\\partial v_1}, \\frac{\\partial C}{\\partial v_2}\\right)^T$$\n",
    "\n",
    "To decrease C, we'll move in the opposite direction of the gradient many times, recomputing the gradient at each step, until we get to the bottom of the valley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### 1. Prove the assertion of the last paragraph. Hint: If you're not already familiar with the [Cauchy-Schwarz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), you may find it helpful to familiarize yourself with it.\n",
    "\n",
    "The assertion is this: \"the choice of $\\Delta v$ which minimizes $\\nabla C \\cdot \\Delta v$ is $\\Delta v=−\\eta \\nabla C$, where $\\eta = \\frac{\\epsilon}{‖\\nabla C‖}$ is determined by the size constraint $‖\\Delta v‖=\\epsilon$\"\n",
    "\n",
    "The Cauchy-Schwarz Inequality, as far as I can tell, just says that the dot product of two vectors is less than or equal to the product of each of those vectors' norms.\n",
    "\n",
    "$|x \\cdot y| <= ||x||*||y||$\n",
    "\n",
    "How might this be used to prove that statement? I guess the situation where $|x \\cdot y|$ is as small as possible is when $x$ and $y$ are pointing in opposite directions. The only time it could be equal to the product of the norms is if $x$ and $y$ are pointing in the same direction.\n",
    "\n",
    "So the vector which points in the opposite direction of $\\nabla C$ is just $-\\eta \\nabla C$, where $\\eta$ is some positive constant. With no size constraint, we would just make $\\eta$ as big as possible to minimize $\\nabla C \\cdot \\Delta v$, but with a huge size constraint, we'd longer be approximating the behavior of the loss function at our point. So we set a size constraint on $\\eta$ by setting it equal to the size constraint divided by the norm of the gradient vector.\n",
    "\n",
    "What affects the magnitude of the norm of the gradient vector? The magnitude of the partial derivatives of each variable with respect to the loss function. If you have a very steep gradient, that means each of the partial derivatives is large, which means the norm of the gradient vector is large, which means $\\eta$ will end up being smaller. So, the steeper the gradient, the less we move along it.\n",
    "\n",
    "####  2. I explained gradient descent when C is a function of two variables, and when it's a function of more than two variables. What happens when C is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?\n",
    "\n",
    "In one dimension, the gradient vector is just the derivative of $C$ with respect to the one variable. That is:\n",
    "\n",
    "$$\\Delta v_1 = -\\eta \\frac{\\partial C}{\\partial v_1}$$\n",
    "\n",
    "If the derivative is negative, that means the tangent line is pointing down and to the right. This means that $\\Delta v_1$ will end up being a positive value, meaning we'll reset $v_1$ so that we're further along to the right, at a higher value for $v_1$. If the derivative is positive, the tangent line is pointing up and to the right. This means that $\\Delta v_1$ will end up being a negative value, meaning we'll reset $v_1$ so that we move back to the left, at a lower value for $v_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
