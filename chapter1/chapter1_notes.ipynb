{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Using neural nets to recognize handwritten digits\n",
    "\n",
    "Starting with definition of artificial neur*ons*, i.e. perceptrons and sigmoid neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons\n",
    "\n",
    "A perceptron is a simple neuron (a node in the neural network) that takes $n$ binary inputs and returns a binary output. It usually attaches weights to each input, and if the incoming signals, multiplied by their respective weights, are higher than a given threshold, the output signal is fired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class perceptron:\n",
    "    def fire(self, inputs, weights, threshold):\n",
    "        return 1 if np.sum(inputs*weights) > threshold else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_neuron = perceptron()\n",
    "my_neuron.fire(\n",
    "    inputs = np.array([1, 1]), \n",
    "    weights = np.array([0.6, 0.5]), \n",
    "    threshold = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By stacking such neurons in multiple layers, the neural network can make increasingly subtle decisions.\n",
    "\n",
    "For notational simplicity that will become clear later on, we use instead of a *threshold* a *bias* term, which describes something like \"the difficulty of getting this neuron to fire.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def fire(self, inputs, weights, bias):\n",
    "        return 1 if np.dot(inputs,weights) + bias > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_new_neuron = perceptron()\n",
    "my_new_neuron.fire(\n",
    "    inputs = np.array([0, 0, 1]), \n",
    "    weights = np.array([2, 2, 6]), \n",
    "    bias = -5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons can be configured to compute logical functions, such as \"and\", \"or\", or \"nand\". For example, here is a \"nand\" perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class nand_perceptron:\n",
    "    weights = np.array([-2, -2])\n",
    "    bias = 3\n",
    "    def fire(self, inputs, weights = weights, bias = bias):\n",
    "        return 1 if np.dot(inputs,weights) + bias > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nand_neuron = nand_perceptron()\n",
    "nand_neuron.fire(inputs = np.array([1, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty cool, because NAND is universal for computation. That is, we can build up any computation out of NAND gates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class classic_nand_gate:\n",
    "    def fire(self, inputs):\n",
    "        return False if inputs[0] and inputs[1] else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_nand_gate = classic_nand_gate()\n",
    "my_nand_gate.fire(inputs = [True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can build up bitwise addition out of NAND gates. And since we can build up NAND out of a perceptron, perceptrons are universal for computation too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_bits(bit1, bit2, type_of_gate):\n",
    "    \n",
    "    if type_of_gate == \"classic\":\n",
    "        gate = classic_nand_gate()\n",
    "    elif type_of_gate == \"perceptron\":\n",
    "        gate = nand_perceptron()\n",
    "    \n",
    "    gate0_in = [bit1, bit2]\n",
    "    gate0_out = gate.fire(inputs = gate0_in)\n",
    "    \n",
    "    gate1_in = [bit1, gate0_out]\n",
    "    gate1_out = gate.fire(inputs = gate1_in)\n",
    "    \n",
    "    gate2_in = [gate0_out, bit2]\n",
    "    gate2_out = gate.fire(inputs = gate2_in)\n",
    "    \n",
    "    gate3_in = [gate1_out, gate2_out]\n",
    "    gate3_out = gate.fire(inputs = gate3_in)\n",
    "    \n",
    "    gate4_in = [gate0_out, gate0_out]\n",
    "    gate4_out = gate.fire(inputs = gate4_in)\n",
    "    \n",
    "    bit_sum = gate3_out\n",
    "    carry_bit = gate4_out\n",
    "    \n",
    "    return str(int(carry_bit)) + str(int(bit_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert add_bits(1, 1, type_of_gate=\"classic\") == '10'\n",
    "assert add_bits(0, 1, type_of_gate=\"classic\") == '01'\n",
    "assert add_bits(1, 0, type_of_gate=\"classic\") == '01'\n",
    "assert add_bits(0, 0, type_of_gate=\"classic\") == '00'\n",
    "\n",
    "assert add_bits(1, 1, type_of_gate=\"perceptron\") == '10'\n",
    "assert add_bits(0, 1, type_of_gate=\"perceptron\") == '01'\n",
    "assert add_bits(1, 0, type_of_gate=\"perceptron\") == '01'\n",
    "assert add_bits(0, 0, type_of_gate=\"perceptron\") == '00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't so exciting on its own. BUT, because perceptrons have weights and bias values that can be tuned to *configure* lower-level logical functions that can in turn be combined to configure higher-level computations, we should be able to expose an artificial neural network to an external source of data that will tune those weights and biases for us. We can then end up with a neural network that is trained to represent any arbitrary computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be able to train our neural network by slightly tuning the weights and biases of its constituent neurons, we need small changes in tunings to cause small changes in outputs. Perceptrons make this hard, because they can only output 0 or 1. There's no middle ground. That's why we'll benefit from using *sigmoid neurons* instead.\n",
    "\n",
    "The output of a sigmoid neuron is not 0 or 1, but $\\sigma(w\\cdot x + b)$, where $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "(In this case $\\sigma()$ is an example of an *activation function* $f()$. We'll see other examples later on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class sigmoid_neuron:\n",
    "    def fire(self, inputs, weights, bias):\n",
    "        return 1 / (1 + math.exp(-np.dot(inputs, weights) - bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01798620996209156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sig_neuron = sigmoid_neuron()\n",
    "my_sig_neuron.fire(inputs=np.array([1, 2, 3]), weights=np.array([1, 1, 1]), bias=-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### Sigmoid neurons simulating perceptrons, part I\n",
    "\n",
    "Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, $c>0$. Show that the behaviour of the network doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_perceptron = perceptron()\n",
    "inputs = np.array([1, 2, 3])\n",
    "weights = np.array([3, 3, 4])\n",
    "bias = -3\n",
    "\n",
    "for positive_constant in range(1, 101):\n",
    "    original = my_perceptron.fire(inputs = inputs, weights = weights, bias = bias)\n",
    "    modified = my_perceptron.fire(inputs = inputs, weights = positive_constant*weights, bias = positive_constant*bias)\n",
    "    assert original == modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a perceptron network is made up of only perceptrons, we can show that the network's overall behaviour will never change so long as any individual perceptron's behaviour will never change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for a perceptron:\n",
    "\n",
    "$1$ if $w\\cdot x + b > 0$\n",
    "\n",
    "$0$ if $w\\cdot x + b <= 0$\n",
    "\n",
    "Let's expand out the dot product and see whether it's possible to change the sign of $cw\\cdot x + cb$ just by varying $c$.\n",
    "\n",
    "$cw\\cdot x + cb = c\\sum_{j}w_{j}x_{j} + cb$\n",
    "\n",
    "$cw\\cdot x + cb = cw_{1}x_{1} + cw_{2}x_{2} + ... + cw_{j}x_{j} + cb$\n",
    "\n",
    "$cw\\cdot x + cb = c(w_{1}x_{1} + w_{2}x_{2} + ... + w_{j}x_{j} + b)$\n",
    "\n",
    "Since $c$ is always a positive constant, the sign of the right-hand side will never change. Therefore, the behaviour of a neural network made up of perceptrons will never change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sigmoid neurons simulating perceptrons, part II\n",
    "\n",
    "Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w\\cdot x+b \\neq 0$ for the input $x$ to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant $c>0$. Show that in the limit as $c\\to\\infty$ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w⋅x+b=0$ for one of the perceptrons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So we're trying to show that this expression behaves exactly like a percetron as $c\\to\\infty$:\n",
    "\n",
    "$\\lim_{c\\to\\infty} \\sigma(cw\\cdot x + cb)$\n",
    "\n",
    "$\\lim_{c\\to\\infty} \\sigma(c(w\\cdot x + b))$\n",
    "\n",
    "$\\lim_{c\\to\\infty} \\frac{1}{1 + e^{c(-w\\cdot x - b))}}$\n",
    "\n",
    "As $c\\to\\infty$, this goes to 0 if $-w\\cdot x - b > 0$, and to 1 if $-w\\cdot x - b < 0$, which is exactly the behaviour of a perceptron. (Technically, the perceptron outputs 0 if $w\\cdot x + b < 0$ and 1 if $w\\cdot x + b > 0$, but these statements are equivalent to the previous ones because $-1*1=-1$ and $-1*-1=1$.)\n",
    "\n",
    "This fails when $w\\cdot x+b=0$ because $\\lim_{c\\to\\infty} \\frac{1}{1 + e^{c*0}} = 0.5$, which is an impossible output for a perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The architecture of neural networks\n",
    "\n",
    "Neural networks which use the output from one layer as the input to the next are called *feedforward* neural networks, and they will be the focus of the book. (See also *recurrent* neural networks, which allow for feedback loops within the network.)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "There is a way of determining the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.\n",
    "\n",
    "![](architecture_exercise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bitwise representations of the numbers from 0 to 9:\n",
    "\n",
    "- 0: 0000\n",
    "- 1: 0001\n",
    "- 2: 0010\n",
    "- 3: 0011\n",
    "- 4: 0100\n",
    "- 5: 0101\n",
    "- 6: 0110\n",
    "- 7: 0111\n",
    "- 8: 1000\n",
    "- 9: 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_output_layer(input_vector, weights_matrix, bias_vector):\n",
    "    return(np.sum(input_vector*weights_matrix, axis=1) + bias_vector > 0)\n",
    "\n",
    "weights_matrix = np.array([[0, 2, 0, 2, 0, 2, 0, 2, 0, 2],\n",
    "                           [0, 0, 2, 2, 0, 0, 2, 2, 0, 0],\n",
    "                           [0, 0, 0, 0, 2, 2, 2, 2, 0, 0],\n",
    "                           [0, 0, 0, 0, 0, 0, 0, 0, 2, 2]])\n",
    "bias_vector = np.array([-1, -1, -1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    0: {\n",
    "        \"input_vector\": np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 0, 0, 0])\n",
    "    },\n",
    "    1: {\n",
    "        \"input_vector\": np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 0, 0, 0])\n",
    "    },\n",
    "    2: {\n",
    "        \"input_vector\": np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 1, 0, 0])\n",
    "    },\n",
    "    3: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 1, 0, 0])\n",
    "    },\n",
    "    4: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 0, 1, 0])\n",
    "    },\n",
    "    5: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 0, 1, 0])\n",
    "    },\n",
    "    6: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
    "        \"output_vector\": np.array([0, 1, 1, 0])\n",
    "    },\n",
    "    7: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
    "        \"output_vector\": np.array([1, 1, 1, 0])\n",
    "    },\n",
    "    8: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
    "        \"output_vector\": np.array([0, 0, 0, 1])\n",
    "    },\n",
    "    9: {\n",
    "        \"input_vector\": np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
    "        \"output_vector\": np.array([1, 0, 0, 1])\n",
    "    }\n",
    "}\n",
    "\n",
    "for digit in range(10):\n",
    "    output = new_output_layer(\n",
    "        input_vector=mappings[digit][\"input_vector\"], \n",
    "        weights_matrix=weights_matrix, \n",
    "        bias_vector=bias_vector\n",
    "    )\n",
    "    expected_output = mappings[digit][\"output_vector\"]\n",
    "    assert (output == expected_output).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
